\documentclass[UTF8]{ctexart}
\usepackage{color}
\usepackage{algorithm}  
\usepackage{algpseudocode}  
\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathrsfs}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\usepackage{tikz}
\usepackage{verbatim}
\usetikzlibrary{trees}

\title{有关BP神经网络的分析及优化改进研究}
\author{余畅 \\ 2019091621002， 信息与软件工程学院}
\begin{document}
\maketitle

\section{摘要}
\textbf{关键词：人工神经网络，BP算法，最速下降法，算法改进} \par
人工神经网络（Artificial Neural Network，ANN ）是20世纪80 年代以来人工智能领域兴起的研究热点。人工神经网络所代表的，即是对人脑的神经元进行不同层次的抽象，使用不同的连接方法组成不同的网络，本质上来说就是一种\textbf{运算模型}。最近十多年来，人工神经网络的研究工作不断深入，在模式识别、预测估计、自动控制等领域都取得了较大的发展。随着研究的不断深入，卷积神经网络，图神经网络，细胞神经网络等算法相继出现。但无法忽视的是，\textbf{BP神经网络}虽然出现时间较早，算法构造相对简单，但其作为人工智能神经网络中的典型算法，其本身具有很强的非线性分类能力，在解决非线性的问题上能力突出。此外，BP神经网络的网络拓扑结构简单，因而具有较高的误差精度；其算法简单，可操作性强，易于编程实现。这些优点都使得BP神经网络成为智能领域的重要算法之一。 \par
然而标准的BP神经网络算法既有其优点，也有其局限性：标准BP神经网络使用最速下降法进行优化，收敛速度慢且很容易陷入局部最小点，产生学习能力不足的问题；标准BP神经网络的隐含层难以通过标准的方法给出所需的层数和每层神经元的节点数，容易产生过拟合和欠拟合的问题；标准BP神经网络是有监督的学习算法，其每一个输入模式都必须知道期望输出以及误差精度。这些缺陷导致标准BP神经网络算法参数过多，经验性较强，算法的学习能力依赖于不断地调参和试错。 \par
本文章旨在介绍标准BP算法的基本原理，结合其不同的缺点总结相应的改进方案，并通过应用改进后的新算法，验证新算法的可行性和鲁棒性。

\section{序言}
\subsection{背景}
人工神经网络是为了进行信息处理而建立的一种数学模型。它被称为神经网络是因为它是在现代神经科学研究成果的基础上提出的，通过一个个人工神经元模拟人类人类大脑神经突触连结形成的网络结构。它可以模仿人脑神经系统对外界信号接受、处理、储存的过程，具有强大的信息处理的能力。 \par
19世纪40年代初，心理学家Mcculloch和数学家Pitts从信息处理和数学建模的角度出发，通过研究信号在神经元之间进行传递的机制，提出了神经元的MP数学模型，随后无导师学习、感知器模型、反向传播算法、最小均方规则、模拟退火算法、竞争学习等等理论和算法也相继被提出。通过这些学习模型，神经网络的学习结构，处理各类信息的能力等都得到了改善，并得到了极大的发展：其具有非线性的映射能力，擅于从输入和输出信号中寻找规律，不需要进行精确的建模，从软硬件层面都易于实现，而且具有并行能力。人工神经网络解决问题的普适性和鲁棒性，使得其在模式识别，人工视觉，文本翻译，信号处理等领域展现出强大的能力。 \par
\subsection{BP神经网络简史}
1985年Rumelhart在\textbf{误差反向传播理论}的基础上提出反向传播（Back-Propagation）学习算法，即BP神经网络算法。BP神经网络构建在多层前馈网络的基础上，由输入、输出、隐含层组成。输入信号首先通过各个神经元激活函数的作用，在各层神经元之间层层传递，最终完成正向传播；接着根据正向传播得到的误差信号，BP神经网络利用梯度下降法进行有导师式的反向传播学习；在正向、反向学习反复不断进行的过程中，误差信号最终收敛到一个最小值。1988年Cybenko指出，当各个节点（即神经元）均采用Sigmoid型函数时，一个隐含层就足以实现任意的判决分类问题，两个隐含层则足以表示输入图形的任意输出函数。这反映出BP神经网络具有强大的数据识别和模拟能力，尤其是在解决非线性系统的问题的领域，主要有：模式识别、智能控制、图像识别、优化计算等等。 \par
\subsection{BP神经网络缺陷}
随着BP神经网络在多领域的广泛应用，BP算法自身存在的诸多问题也逐渐暴露出来：BP神经网络的收敛速度慢，导致学习的时间过长；学习过程中容易陷入局部极小值；网络的泛化能力差；构建网络结构缺乏统一原则等。缺陷产生的直接原因是BP神经网络自身的算法不完善，产生了算法精度差的直接后果，影响网络的学习速度，限制了网络的广泛运用。自从BP算法被提出之后，对于它的改进工作就一直在进行。本文希望能通过分析BP算法的原理之后，针对BP算法的不足，借鉴前人的一些实际运用的改进经验，进一步提出改进BP算法的新思路。

\section{BP神经网络的原理及方法}

\subsection{BP神经网络的基本原理}

作为人工神经网络中应用广泛的算法模型，BP神经网络具有完备的理论体系和学习机制。BP神经网络模仿人脑神经元对外部激励信号的反应过程，建立多层的感知器模型，并且不断地重复迭代信号的正向传播和误差的反向调节，通过这样多次的迭代学习，网络将自动调整收敛到最终的结果。

\subsection{BP神经网络的理论基础}

BP神经网络的结构基础是\textbf{单层感知器模型}，作为神经网络中的一个最小的单元，它试图模拟人脑神经元记忆、学习和认知的过程。现代生理学研究证明，人脑的大脑皮层的神经细胞数量约在10亿的级别上，而大约有60万亿左右的神经突触以及连接体。单一的生物神经元并不能完成人类复杂的反射活动，但也不是简单粗暴的将多个生物神经元在功能上简单叠加就可以做到，事实上，人类的大脑是\textbf{一系列神经元进行大量非线性动态数据处理而形成的信息处理系统}。相似的，人工神经元采用阈值激活函数（Activation function）将一组输入向量映射到一个 $0$ 或是 $1$ 的目标输出。人脑的神经元形态各异，但是具有共性的部分，主要包括四个部分：细胞体、树突、轴突和突触。树突是细胞向外延伸出的数量较多的较短分支，可以接受其他神经元提供的脉冲信息，相当于细胞的输入端；轴突是细胞单一的向外伸出的最长分支，信息通过此传输到其他的神经元，相当于细胞的输出端。总结来说，\textbf{信息流从树突出发，经过细胞体，最终从轴突传出}。 \par

通常来看，构成BP神经元模型的三个要素是：

\begin{itemize}
\item [1)]
具有一组突触或连接。常用 $w_{ij}$ 表示神经元 $i$ 和神经元 $j$ 之间的连接强度（权值）。和生物神经元一样，这个权值可正可负。若正，则表示突触起到激活的作用；反之为负，则起到抑制的作用。如果突触 $j$ 接受到了强度为 $s_i$ 的输入信号，则信号值会和突触 $j$ 上的权重 $w_{ji}$ 相乘。
\item [2)]
具有反映生物神经元时空整合功能的输入信号累加器。即将输入权值向量通过不同的权重累加映射为实数，可表达为 $R_j = \sum_{i=1}^{n}w_{ji}x_{i}$
\item [3)]
具有一个激活函数用于限制神经元输出。激活函数通常将输出范围限制在 $[-1,1]$ 或者 $[0,1]$ 的实数域中。
\end{itemize}

1943年心理学家 McCelland 和数学家 W. Pitts 通过分析人脑神经系统结构，在此研究的基础上，提出了MP模型。1986年以Rumelhart，Williams，Hinton，McCelland等科学家为首的科学家小组提出了BP（Back Propagation）神经网络，其定义是由反向传播误差算法训练出的多层前馈网络。总结来看，BP神经元模型就是MP模型不断改进之后的结果。BP神经网络可以灵活地学习和储存输入和输出之间的映射关系而无需在学习和储存前事先通过某种数学方程或者算法描述出这种映射关系。最速下降法是BP神经网络的学习算法，网络在正向传播之后计算误差，通过将误差反向传播的方式调整网络的连接权系数和阈值的信息，最终使得神经网络的平方误差最小，达到期望要求。 \par

BP（Back Propagation）神经网络的基本原理是把一个输入矢量（由训练样本提供）经过隐含层的一系列变换，然后得到一个输出矢量，从而实现输入数据与输出数据间的一个映射关系。输入信息的正向传播，以及输出误差的反向传播，构成了BP网络的信息循环。BP算法根据输出误差来修改各神经元连接的连接权系数，其目的是使输出误差达到期望的范围内。BP网络是一种有导师学习网络，因为，它需要实际输出与期望输出之间的误差来确定是否要修改神经元的连接权系数。其中，期望输出便是该网络意义上的“导师”。BP网络具有对称性的网络结构。在BP网络中，输出端的每一个处理单元基本上都具有一个相同的激励函数。

\subsubsection{BP神经网络的拓扑结构}

BP神经网络模型的拓扑结构可以被划分为三层：输入层（Input Layer），输出层（Output Layer）和隐含层（Hide Layer）。其中输入层和输出层是必要的，隐含层可以划入输入层或者去掉。和生物神经元类似，拓扑层中的每一层都
由大量能够执行并行运算的简单神经元组成。理论上讲每一层中的单个神经元的运算都是并行进行的，所以神经网络的复杂度只和层数有关，但由于计算机算力的限制，实际情况下人工神经元的并行性不可能达到生物神经元的并行性。BP神经网络是一个前馈网络，因此它具有前馈网络所具有的特性，即\textbf{相邻两层之间的全部神经元都进行相互连接，而处于同一层的神经元之间无法连接}。虽然单一神经元的结构简单且功能有限，但是一旦将数量庞大神经元构建为一定连接方式的网络系统，它们之间相互交互和传递，记录数据，就可以解决许多复杂度问题。神经网络能够对信息进行并行协同处理并行协同处理并分布式储存信息，由此可以看出它是一种非线性动力学系统，上面描述的就是这种系统的特点。 \par

在BP网络的拓扑结构中，网络层数以及输入节点与输出节点的节点数目都是待解决问题本身和问题规模来确定的，最关键的是对隐含层的层数与隐含层中节点数目的确定。 \par

隐含层是神经网络中一层或多层位于输入层和输出层之间的中间层，可以被看作是输入模式在神经网络中的一种内部表示。它的作用是把一类输入模式中与其他类输入模式不相同的特征进行抽取，并且将抽出的那部分特征再次传递给输出层，然后由输出层对其做出判断。隐含层产生作用的过程，即抽取输入模式特征的过程，实际上就是实现了\textbf{输入层与隐含层的连接权系数的调整过程}，是一个“自组织化的过程”。因此，在网络的学习训练过程中，层与层间的连接权系数起着输入层到输出层之间桥梁的作用，即用于“特征的传递”。

\subsubsection{BP神经网络的学习过程}

BP神经网络的学习算法实际上就是采用最速下降法对误差函数求极小值（而不是最小值）的算法，它通过反复迭代的训练过程，通过反向传播误差来修改连接权系数，它是沿着输出误差函数的负梯度方向对其进行改变。理想状态下，算法应使误差函数最终收敛到该函数的最小点，但由于梯度只是函数的局部特征，如果没有控制正确的算法步长（即学习速率，算法最后往往收敛到的是函数的极小值而不是最小值。通常情况下，需要随机多次选取不同的初始值以得到最好的结果。

\subsection{BP神经网络的基本原理}

\subsubsection{BP神经元原理}

BP神经元和BP神经网络的关系就像蜂群和每只蜜蜂的关系一样。很难说每只蜜蜂是不可或缺的，或者说会对整个群体产生重大影响，但是其构成的整体就产生了质变的效果，BP神经元是构成BP神经网络的基本单元。因此，如果要了解BP神经网络的基本原理，那么从BP神经元的基本原理入手将会产生更好的效果。 \par

和人工神经元类似，BP神经元主要模仿了生物神经元的三个最基本但是是最重要的特征：加权、求和和转移。令 $x_1,x_2,...,x_i,...,x_n$ 分别代表来自神经元 $1,2,...,i,...,n$ 的输入；$w_{j1},w_{j2},...,w_{jn}$ 这一向量表示的是网络前一层神经元 $1,2,...,i,...,n$ 与这一层的第 $j$ 个神经元连接权系数；$b_j$ 为阈值；$f(.)$ 为传递函数； $y_j$ 为第 $j$ 个神经元的输出。可以推知 BP 网络在第 $j$ 个神经元的净输入值可以表示为：

\begin{equation} 
S_j = \sum_{i=1}^n w_{ji} \cdot x_i + b_j = W_j X + b_j
\end{equation}

其中，$X = [x_1x2...x_i...x_n]^T$，$W=[w_{j1}w_{j2}...w_{ji}...w_{jn}]$。如果让 $x_0=1$，$w_{j0}=b_j$，即令 $X$ 及 $W_j$ 包括 $x_0$ 和 $w_{j0}$，则：

\begin{equation} 
X=[x_0x_1x_2...x_i...x_n]^T,W=[w_{j0}w_{j1}w_{j2}...w_{ji}...w_{jn}]
\end{equation}

那么，$j$ 节点的净输入 $S_j$ 可以用下面的公式来表示：

\begin{equation} 
S_j = \sum_{i=0}^n w_{ji} \cdot x_i = W_j X
\end{equation}

净输入 $S_j$ 通过激励函数（Transfer Function）$f(.)$ 之后，就可以得到第 $j$ 个神经元的净输出 $y_j$：

\begin{equation} 
y_j = f(S_j) = f(\sum_{i=0}^n w_{ji} \cdot x_i) = f(W_j X)
\end{equation}

在上式中，$f(.)$ 是一个单调递增的有界函数，由于其存在上界，所以神经元细胞所传递的信号不会被无限放大。

\subsubsection{BP网络算法原理}

BP算法能够比较系统地解决多层前馈网络中隐含层神经元地连接权系数的许多学习问题。 \par

(1) 正向传播 \par

前文已对BP算法的正向传播进行了简单的描述。关于BP算法的正向传播，其传播方向为先从输入层到隐含层，再通过隐含层传输到输出层，并且每一层的神经元状态仅仅影响到它下一层的神经元，同层的神经元则不会相互影响。如果在输出层得到的实际输出不能满足我们所期往的输出，那么BP算法就会转到误差的反向传播过程中。正向传播和反向传播这两个过程是轮流进行的。 \par

不妨设BP网络的输入层、隐含层和输出层三层的节点个数分别记为:$n$、$q$、$m$；每个神经元节点（单元）均使用Sigmoid型函数作为激励函数；并用 $v_{ki}$ 来表示输入层和隐含层之间的连接权系数；而用 $w_{jk}$ 来表示隐含层与输出层之间的连接权系数。假设隐含层和输出层的激励函数分别为 $f_1(.)$，$f2_(.)$，那么当输入为 $w_i,i=0,1,...,n$ 时，隐含层神经元节点的输出则为：

\begin{equation} 
c_k = f_1(\sum_{i=0}^{n} v_{ki} \cdot x_i) k = 1,2,...,q
\end{equation}

输出端的神经元节点的输出方程为：

\begin{equation} 
y_j = f_2(\sum_{k=0}^{q} w_{jk} \cdot c_k) j = 1,2,...,m
\end{equation}

根据上述两个公式，我们就可以得到一次BP神经网络的近似输出，也就是说，通过以上操作可以完成一个非线性的近似映射，这个映射是由 $n$ 维空间向量向 $m$ 维空间向量的一个映射。

(2) 反向传播 \par

要学习BP网络的反向传播，首先要定义该网络的误差函数。接下来就先确定一个误差函数。 \par

首先输入 $P$ 个学习样本，分别用 $x^1,x^2,...,x^p$ 来表示这些样本。把这些学习样本输入到神经网络中后，我们便可以得到一个实际输出$y_j^p(j=1,2,...,m)$。如果把平方型误差函数作为该神经网络的目标函数，那么再计算以后我们便可以得到第 $p$ 个学习样本的误差 $E_p$：

\begin{equation} 
E_p=\frac{1}{2} \sum_{j=1}^{m}  (t_{j}^{p} - y_{j}^{p})^2
\end{equation}

其中，$t_{j}^{p}$ 为我们定义的一个期望输出。对于这 $P$ 个学习样本来说，它们的全局误差计算公式如下：

\begin{equation} 
E=\frac{1}{2} \sum_{p=1}^{P} \sum_{j=1}^{m}  (t_{j}^{p} - y_{j}^{p})^2 = \sum_{p=1}^{P} E_p
\end{equation}

计算BP网络中各个样本的误差的目的主要就是为了改变网络权值，使其更加适合、更能满足网络需求，并且使误差减小到精度要求。那么误差的计算是如何去改变权值的呢？下面将对这个过程做出详细的描述。 \par

如果利用累计误差方法来计算BP算法的误差，并用此来调整连接权系数 $w_{jk}$，那么就可以很大程度上减小全局误差 $E$，其计算公式如下：

\begin{equation} 
\Delta w_{jk} = - \eta \frac{\partial E}{\partial w_{jk}} = - \eta \frac{\partial}{\partial w_{jk}} (\sum_{p=1}^{P} E_p) = \sum_{p=1}^{P} (- \eta \frac{\partial E_p}{\partial w_{jk}})
\end{equation}

其中，$0<\eta<1$，它通常被称为学习速率。 \par

误差信号可定义为：

\begin{equation}
\delta_{yj} = - \frac{\partial E_p}{\partial S_j} = - \frac{\partial E_p}{\partial y_j} \cdot \frac{\partial y_j}{\partial S_j}
\end{equation}

对上述公式中的第一项做如下的分析：

\begin{equation}
\frac{\partial E_p}{\partial y_j} = \frac{\partial}{\partial y_j} [\frac{1}{2} \sum_{j=1} ^ {m} (t_{j}^{p} - y_{j}^{p})^2] = - \sum_{j=1}^{m} (t_{j}^{p} - y_{j}^{p})
\end{equation}

上式中的第二项则可被分析为：

\begin{equation}
\frac{\partial y_j}{\partial S_j} = f_2^{'}(S_j)
\end{equation}

根据上式可知这一项是输出层激活函数的偏微分。 \par

所以误差可以用下式表示： \par

\begin{equation}
\delta_{yj} = \sum_{j=1}^{m} (t_{j}^{p} - y_{j}^{p}) \cdot f_2^{'}(S_j)
\end{equation}

根据链式法则可以得到：

\begin{equation}
\frac{\partial E}{\partial w_{jk}} = \frac{\partial E}{\partial S_j} \cdot \frac{\partial S_j}{\partial w_{jk}} = - \delta_{yj} \cdot c_k = - \sum_{j=1}^{m} (t_{j}^{p} - y_{j}^{p}) \cdot f_2^{'}(S_j) \cdot c_k
\end{equation}

综上所述，我们可以使用下公式来表示输出层每一个神经元的连接权系数的调整：

\begin{equation}
\Delta w_{jk} = \sum_{p=1}^{P} \sum_{j=1}^{m} \eta (t_{j}^{p} - y_{j}^{p}) \cdot f_2^{'}(S_j) \cdot c_k
\end{equation}

上面介绍了输出层各神经元的权值调整，接下来介绍隐含层的权值是如何变化的。 \par

隐含层权值变化与全局误差的关系可以用下式表示：

\begin{equation}
\Delta v_{ki} = - \eta \frac{\partial E}{\partial v_{ki}} = - \eta \frac{\partial}{\partial v_{ki}} (\sum_{p=1}^{P} E_p) = \sum_{p=1}^{P} (- \eta \frac{\partial E}{\partial v_{ki}})
\end{equation}

信号误差可以定义为：

\begin{equation}
\delta_{ck} = -\frac{\partial E_p}{\partial S_k} = -\frac{\partial E_p}{\partial c_k} \cdot \frac{\partial c_k}{\partial S_k}
\end{equation}

上述公式中的第一项可以做如下分析：

\begin{equation}
\frac{\partial E_p}{\partial c_k} = \frac{\partial}{\partial c_k} [\frac{1}{2} \sum_{j=1}^{m}  (t_{j}^{p} - y_{j}^{p})^2] = - \sum_{j=1}^{m} (t_{j}^{p} - y_{j}^{p}) \frac{\partial y_j}{\partial c_k}
\end{equation}

根据链式求导法则，我们可以得到：

\begin{equation}
\frac{\partial y_j}{\partial c_k} = \frac{\partial y_j}{\partial S_j} \cdot \frac{\partial S_j}{\partial c_k} = f_{2}^{'}(S_j)w_{jk}
\end{equation}

对于公式 $(17)$ 的第二项，则有如下分析：

\begin{equation}
\frac{\partial c_k}{\partial S_k} = f_{1}^{'}(S_k)
\end{equation}

很明显，第二项为隐含层激励函数的偏微分。 \par

因此，隐含层权值变化可以表示为：

\begin{equation}
\delta_{ck} = \sum_{j=1}^{m} (t_{j}^{p} - y_{j}^{p}) \cdot f_2^{'}(S_j) \cdot w_{jk} \cdot f_1^{'}(S_k)
\end{equation}

根据链式求导法则，我们可以得到：

\begin{equation}
\frac{\partial E_p}{\partial v_{ki}} = \frac{\partial E_p}{\partial S_k} \cdot \frac{\partial S_k}{\partial v_{ki}} = - \delta_{ck} \cdot x_i = - \sum_{j=1}^{m} (t_{j}^{p} - y_{j}^{p}) \cdot f_2^{'}(S_j) \cdot w_{jk} \cdot f_1^{'}(S_k) \cdot x_i
\end{equation}

综上所述，可以用以下公式来表示隐含层的每一个神经元的连接权系数调整：
\begin{equation}
\Delta v_{jk} = \sum_{p=1}^{P} \sum_{j=1}^{m} \eta (t_{j}^{p} - y_{j}^{p}) \cdot f_2^{'}(S_j) \cdot w_{jk} \cdot f_1^{'}(S_k) \cdot x_i
\end{equation}

\subsubsection{BP算法的步骤}

上一节主要介绍了BP算法的基本原理，下面将简要介绍BP算法的具体步骤。 \par

BP算法的具体步骤如下：

\begin{itemize}
\item [1)]
权值初始化：随机地给 $w_{mi}(0)$、$w_{ij}(0)$、$w_{jp}(0)$ 赋值一组较小的非零数值。
\item [2)]
确定BP神经网络地结构参数并给出相关变量的定义：设输入向量为 $X_k = [x_{k1},x_{k2},...,x_{km}],(k=1,2,...,n$，该网络的训练样本的个数为 $n$。$Y_k(n)=[y_{k1}(n),y_{k2}(n),...,y_{kp}(n)]$ 为BP神经网络进行第 $n$ 次迭代后的实际输出。$d_k = [d_{k1},d_{k2},...,d_{kp}]$ 为期望得到的输出。
\item [3)]
输入训练样本：依次输入训练样本集 $X=[X_1,X_2,...,X_p]$，假设这次学习的样本为 $X_k(k=1,2,...,n$。
\item [4)]
正向传播过程：根据给定的训练模式输入，计算出网络的输出模式，并将其与期望模式进行比较，如果存在误差就执行 $(5)$；否则返回 $(6)$
\item [5)]
反向传播过程：a、计算同一层单元的误差；b、修正权值和阈值；c、返回 $(3)$，如果误差满足要求，则执行 $(6)$。
\item [6)]
训练结束。
\end{itemize}

\subsection{本章小结}

本章概述了BP神经网络的基本概念和工作原理，并详细阐述了它所使用的算法。在这章中首先对BP神经网络的拓扑结构进行了详细的描述，并在此基础上对它的学习过程做了进一步的解析；其次BP概述了神经元的基本原理；最后根据BP神经元的基本原理推导出了BP算法的基本工作原理，并对BP算法的训练过程进行了总结。

\section{BP神经网络算法的改进}

\subsection{BP算法的优缺点}

标准BP网络模型把训练样本的输入输出问题转变为了一个有关非线性的数学优化问题。根据Widrow-Hoff规则，BP算法在学习算法方面使用了最速下降算法，并且对问题尤其是应用问题具有很强的识别功能。由于强大的通用性，识别和控制非线性系统的功能强大，BP算法成为应用最为广泛的人工神经网络之一。理论上来讲，BP神经网络对于任意复杂的非线性模型仿真，它的误差都可以达到任意小的程度；根据不同的具体情况，BP网络可以任意的设定网络的中间层数、各层的节点数及网络的学习速率等参数，由此可见，BP神经网络具有较大的灵活性。 \par

由分析可知，\textbf{只要BP神经网络的隐含层层数或者隐含层的神经元节点数足够多，那么网络就可以完成对任意非线性映射的逼近}。BP算法的学习算法是一个全局逼近的优化算法，因此它具有良好的泛化能力和较强的容错性。但是其仍存在以下缺陷： \par

\item [1)]
收敛速度慢。在学习过程中，学习速率的收敛情况比较慢。尤其是网络的训练到达了一定程度后。举例来说，当误差下降到一定程度后，网络经过8000次训练，可能在这个过程中累加得到的误差下降量还不到0.001。此外，标准BP算法在误差下降时，难免会产生振荡的现象，这会严重影响网络的收敛速度。对于某些复杂问题，BP算法可能要进行几个小时甚至更长时间的学习训练。
\item [2)]
目前来看仍没有给出网络隐含层层数和网络层节点数选取的理论指导，只能根据实际经验来估计，然后在实践的过程中慢慢调整，在实际应用中，遵循着不同的结构确定原则，验证所选结构的合理性又需要大量的仿真实验。正因为如此，BP网络有时候并不能取得最佳的设计。一方面在网络过大时，容易产生学习效率低下或者过拟合导致容错性下降的问题；另一方面如果网络太小，则误差函数有可能不存在收敛性。
\item [3)]
BP网络的学习误差在下降过程中常常会处于停滞状态。即在学习过程中，很容易陷入局部极小值。BP网络是沿着一个局部的方向来逐渐改善网络的极值，然后希望使输出的误差函数达到最小化的一组全局解，但
\item [4)]
正向传播过程：根据给定的训练模式输入，计算出网络的输出模式，并将其与期望模式进行比较，如果存在误差就执行 $(5)$；否则返回 $(6)$
\item [5)]
反向传播过程：a、计算同一层单元的误差；b、修正权值和阈值；c、返回 $(3)$，如果误差满足要求，则执行 $(6)$。


\end{document}